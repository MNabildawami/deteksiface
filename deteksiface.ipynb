{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17M3r4H0jAnMy9SaJL94rUFqe4lf7USeY",
      "authorship_tag": "ABX9TyMhi6yAYV6i9cA/O2bpxkkb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MNabildawami/deteksiface/blob/main/deteksiface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyheif\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfFKOsheWQ7P",
        "outputId": "79df3f60-c701-4bda-e547-31ff7e5426f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyheif\n",
            "  Downloading pyheif-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyheif) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->pyheif) (2.22)\n",
            "Downloading pyheif-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyheif\n",
            "Successfully installed pyheif-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuNOnmjHWstT",
        "outputId": "fdb7ff0a-dabc-42a4-f5ba-828c6b13b537"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTIVlSz2Pk1S",
        "outputId": "b006225a-c9c8-413a-8117-44aa28af3e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-12 19:32:13--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2.12’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  1.64MB/s    in 20s     \n",
            "\n",
            "2024-12-12 19:32:33 (3.09 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2.12’ saved [64040097/64040097]\n",
            "\n",
            "bzip2: Output file shape_predictor_68_face_landmarks.dat already exists.\n",
            "Akurasi model SVM: 100.00%\n",
            "Data landmark berhasil disimpan dalam file Excel: landmarks_output.xlsx\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pyheif\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Unduh dan ekstrak model landmark wajah\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "# Muat model detektor wajah dlib\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# Muat model prediktor landmark wajah (pre-trained model)\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "# Fungsi untuk mengekstrak landmark dari wajah dan mengembalikan sebagai list data\n",
        "def extract_face_landmarks_data(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    # Periksa apakah wajah terdeteksi\n",
        "    if len(faces) == 0:\n",
        "        print(\"Tidak ada wajah yang terdeteksi.\")\n",
        "        return None\n",
        "\n",
        "    landmarks_data = []  # List untuk menyimpan data landmark dari satu gambar\n",
        "\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "        # Loop melalui 68 titik landmark\n",
        "        for n in range(0, 68):\n",
        "            x = landmarks.part(n).x\n",
        "            y = landmarks.part(n).y\n",
        "            landmarks_data.append((x, y))  # Tambahkan koordinat (x, y) ke list\n",
        "    return landmarks_data\n",
        "\n",
        "# Fungsi untuk membaca gambar HEIC dan mengkonversinya menjadi array NumPy\n",
        "def read_heic_image(heic_file):\n",
        "    heif_file = pyheif.read(heic_file)\n",
        "    image = Image.open(io.BytesIO(heif_file.data))\n",
        "    return np.array(image)\n",
        "\n",
        "# Fungsi untuk memproses semua gambar dalam folder dan mengembalikan dataframe dengan landmark\n",
        "def process_folder_images(folder_path):\n",
        "    data = []\n",
        "    labels = []  # Menyimpan label untuk setiap gambar\n",
        "\n",
        "    # Loop untuk setiap file gambar di folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Cek apakah file berformat HEIC atau format gambar lain\n",
        "        if file_name.lower().endswith(('.jpg', '.jpeg', '.png', '.heic')):  # Filter hanya file gambar\n",
        "            # Jika gambar dalam format HEIC\n",
        "            if file_name.lower().endswith('.heic'):\n",
        "                image = read_heic_image(file_path)\n",
        "            else:\n",
        "                image = cv2.imread(file_path)\n",
        "\n",
        "            # Cek apakah gambar berhasil dimuat\n",
        "            if image is None:\n",
        "                print(f\"Gambar {file_name} tidak ditemukan atau tidak bisa dimuat!\")\n",
        "                continue\n",
        "\n",
        "            # Ekstraksi fitur geometri sebagai data\n",
        "            landmarks_data = extract_face_landmarks_data(image)\n",
        "\n",
        "            if landmarks_data is not None:\n",
        "                # Tambahkan hasil ekstraksi ke list data, termasuk nama file\n",
        "                flattened_landmarks = [coord for point in landmarks_data for coord in point]  # Flatten koordinat\n",
        "                data.append(flattened_landmarks)\n",
        "\n",
        "                # Asumsi bahwa label berdasarkan folder atau nama file\n",
        "                label = file_name.split('_')[0]  # Menetapkan label berdasarkan bagian nama file\n",
        "                labels.append(label)\n",
        "\n",
        "    # Periksa apakah ada lebih dari satu kelas di label\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        raise ValueError(\"Data latih hanya memiliki satu kelas. Harap pastikan ada lebih dari satu kelas.\")\n",
        "\n",
        "    # Buat kolom untuk dataframe\n",
        "    column_names = [f'x{i+1}' for i in range(68)] + [f'y{i+1}' for i in range(68)]\n",
        "\n",
        "    # Buat dataframe dari data\n",
        "    df = pd.DataFrame(data, columns=column_names)\n",
        "\n",
        "    return df, labels\n",
        "\n",
        "# Tentukan folder yang berisi gambar\n",
        "folder_path = '/content/drive/MyDrive/footage item pengpol /nabil'  # Ganti dengan path folder Anda\n",
        "\n",
        "# Proses semua gambar di folder dan simpan hasil ekstraksi ke dataframe\n",
        "landmarks_df, labels = process_folder_images(folder_path)\n",
        "\n",
        "# Membagi data latih dan data uji (70% data latih dan 30% data uji)\n",
        "X_train, X_test, y_train, y_test = train_test_split(landmarks_df, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Latih model SVM dengan data latih\n",
        "svm_classifier = SVC(kernel='linear')  # Gunakan kernel linear untuk SVM\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi label untuk data uji\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Tampilkan hasil pengujian dan akurasi\n",
        "print(f\"Akurasi model SVM: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Simpan hasil ekstraksi ke file CSV\n",
        "output_csv = 'landmarks_output.csv'\n",
        "landmarks_df.to_csv(output_csv, index=True)\n",
        "\n",
        "# Simpan hasil ekstraksi ke file Excel\n",
        "output_excel = 'landmarks_output.xlsx'\n",
        "landmarks_df.to_excel(output_excel, index=True)  # Simpan ke Excel\n",
        "\n",
        "# Tampilkan pesan sukses\n",
        "print(f\"Data landmark berhasil disimpan dalam file Excel: {output_excel}\")\n"
      ]
    }
  ]
}